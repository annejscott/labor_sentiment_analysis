{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".csv with all manual labels applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/master/reddit_labelled-sample.csv\"\n",
    ")\n",
    "reddit = reddit.dropna(subset=['label'])\n",
    "reddit = f.reddit_dtypes(reddit)\n",
    "\n",
    "# Remove the word 'deleted' from the 'text' column\n",
    "reddit['text'] = reddit['text'].str.replace('deleted', '', regex=False)\n",
    "\n",
    "# Save the dataframe as a pickle object\n",
    "pickle_folder = \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/pickle\"\n",
    "pickle_path_reddit = os.path.join(pickle_folder, \"reddit_labelled-sample.pkl\")\n",
    "joblib.dump(reddit, pickle_path_reddit)\n",
    "\n",
    "reddit.to_csv(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/master/reddit_labelled-sample.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 1598, Length of y: 1598\n"
     ]
    }
   ],
   "source": [
    "# features and target\n",
    "X = reddit[\"text\"].apply(f.token_and_lemmatize_nb)\n",
    "y = reddit[\"label\"]\n",
    "\n",
    "print(f\"Length of X: {len(X)}, Length of y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1598, 1000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"X shape: {X_vectorized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/pickle/X_vectorized.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export X_vectorized as a pickle object\n",
    "pickle_path = os.path.join(pickle_folder, \"X_vectorized.pkl\")\n",
    "joblib.dump(X_vectorized, pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# encode target\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "print(f\"Classes: {encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1278, 1000) Test shape: (320, 1000)\n",
      "Training set class distribution:\n",
      "1    0.418623\n",
      "2    0.404538\n",
      "0    0.176839\n",
      "Name: proportion, dtype: float64\n",
      "Test set class distribution:\n",
      "1    0.41875\n",
      "2    0.40625\n",
      "0    0.17500\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectorized, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}\", f\"Test shape: {X_test.shape}\")\n",
    "print(\"Training set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Test set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.5}\n",
      "Best score: 0.6392830882352941\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    \"alpha\": [0.001, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0]\n",
    "}\n",
    "\n",
    "# grid search w/ 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=nb, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring=\"accuracy\")\n",
    "\n",
    "# fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.621875\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.09      0.15        56\n",
      "     neutral       0.68      0.70      0.69       134\n",
      "    positive       0.58      0.77      0.66       130\n",
      "\n",
      "    accuracy                           0.62       320\n",
      "   macro avg       0.59      0.52      0.50       320\n",
      "weighted avg       0.61      0.62      0.58       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train model with best parameters\n",
    "custom_nb = grid_search.best_estimator_\n",
    "custom_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = custom_nb.predict(X_test)\n",
    "\n",
    "# decode\n",
    "custom_nb_pred = encoder.inverse_transform(y_pred)\n",
    "\n",
    "y_test_decoded = encoder.inverse_transform(y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test_decoded, custom_nb_pred)\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_decoded, custom_nb_pred, \n",
    "                            target_names=encoder.classes_,\n",
    "                            zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if stratified sampling creates a stronger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "negative    282\n",
      "neutral     282\n",
      "positive    282\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/hb3zcj293r197l8g8x2s58900000gn/T/ipykernel_83701/339534131.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reddit_stratified = reddit.groupby(\"label\", observed=True).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 846, Length of y: 846\n",
      "X shape: (846, 1000)\n",
      "Best parameters: {'alpha': 2.0}\n",
      "Best score: 0.578355119825708\n",
      "Stratified set accuracy: 0.6\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.62      0.57        56\n",
      "     neutral       0.72      0.58      0.64        57\n",
      "    positive       0.59      0.60      0.59        57\n",
      "\n",
      "    accuracy                           0.60       170\n",
      "   macro avg       0.61      0.60      0.60       170\n",
      "weighted avg       0.61      0.60      0.60       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling to keep an even distribution of levels in the 'label' column\n",
    "min_count = reddit[\"label\"].value_counts().min()\n",
    "reddit_stratified = reddit.groupby(\"label\", observed=True).apply(\n",
    "    lambda x: x.sample(min_count, random_state=42).reset_index(drop=True)\n",
    ")\n",
    "print(reddit_stratified[\"label\"].value_counts())\n",
    "\n",
    "# features and target\n",
    "X_strat = reddit_stratified[\"text\"].apply(f.token_and_lemmatize_nb)\n",
    "y_strat = reddit_stratified[\"label\"]\n",
    "\n",
    "print(f\"Length of X: {len(X_strat)}, Length of y: {len(y_strat)}\")\n",
    "\n",
    "# vectorize\n",
    "X_strat_vectorized = vectorizer.fit_transform(X_strat)\n",
    "\n",
    "print(f\"X shape: {X_strat_vectorized.shape}\")\n",
    "\n",
    "y_strat = encoder.fit_transform(y_strat)\n",
    "\n",
    "# split data\n",
    "X_strat_train, X_strat_test, y_strat_train, y_strat_test = train_test_split(\n",
    "    X_strat_vectorized, y_strat, \n",
    "    test_size=0.2,\n",
    "    random_state=572, \n",
    "    stratify=y_strat\n",
    ")\n",
    "\n",
    "# grid search w/ 5-fold cross-validation\n",
    "grid_search_strat = GridSearchCV(\n",
    "    estimator=nb, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "grid_search_strat.fit(X_strat_train, y_strat_train)\n",
    "\n",
    "# best parameters and score\n",
    "print(f\"Best parameters: {grid_search_strat.best_params_}\")\n",
    "print(f\"Best score: {grid_search_strat.best_score_}\")\n",
    "\n",
    "# train model with best parameters\n",
    "strat_nb = grid_search_strat.best_estimator_\n",
    "strat_nb.fit(X_strat_train, y_strat_train)\n",
    "\n",
    "# predict\n",
    "y_strat_pred = strat_nb.predict(X_strat_test)\n",
    "\n",
    "# decode\n",
    "strat_pred = encoder.inverse_transform(y_strat_pred)\n",
    "\n",
    "y_strat_test_decoded = encoder.inverse_transform(y_strat_test)\n",
    "\n",
    "accuracy = accuracy_score(y_strat_test_decoded, strat_pred)\n",
    "print(f\"Stratified set accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_strat_test_decoded, strat_pred, \n",
    "        target_names=encoder.classes_, \n",
    "        zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because stratifying reduces sample size so much, adding it does not strengthen the model by adding more balanced target to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.36\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.20      0.47      0.28       282\n",
      "     neutral       0.41      0.12      0.18       669\n",
      "    positive       0.49      0.57      0.53       647\n",
      "\n",
      "    accuracy                           0.36      1598\n",
      "   macro avg       0.37      0.39      0.33      1598\n",
      "weighted avg       0.40      0.36      0.34      1598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply VADER to test set\n",
    "reddit_with_vader = f.vader_analysis(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward with my customized naive bayes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nb_label_encoder.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "models_folder = \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models\"\n",
    "model_path = os.path.join(models_folder, \"custom_nb_model.pkl\")\n",
    "joblib.dump(custom_nb, model_path)\n",
    "\n",
    "# Save the encoder\n",
    "encoder_path = os.path.join(models_folder, \"nb_label_encoder.pkl\")\n",
    "joblib.dump(encoder, encoder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
