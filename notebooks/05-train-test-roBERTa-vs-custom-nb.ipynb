{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".csv with all manual labels applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "reddit = joblib.load('/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/pickle/reddit_labelled-sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "reddit[\"processed_text\"] = reddit[\"text\"].apply(f.token_and_lemmatize_rob)\n",
    "\n",
    "# Features and labels\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(reddit[\"processed_text\"])  # Tfidf vectorization\n",
    "y = reddit[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=13,\n",
    "    stratify=y\n",
    ")\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test RoBERTa (Robustly Optimized BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense and prepare datasets\n",
    "X_train_text = [\" \".join(map(str, row)) for row in X_train.toarray()]\n",
    "X_test_text = [\" \".join(map(str, row)) for row in X_test.toarray()]\n",
    "\n",
    "train_data = {\"text\": X_train_text, \"labels\": y_train.tolist()}\n",
    "test_data = {\"text\": X_test_text, \"labels\": y_test.tolist()}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize using Hugging Face tokenizer\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742eeebf15bb41c7917394f695841046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a744a3f9e5a4759bc388e4ded5de373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(f.tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(f.tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/seshat/Documents/GitHub/labor_sentiment_analysis/venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/nx/hb3zcj293r197l8g8x2s58900000gn/T/ipykernel_91276/2013619812.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d7b3e69abd4c89831c096bcf3f614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0246, 'grad_norm': 6.180790901184082, 'learning_rate': 1.9494949494949496e-05, 'epoch': 0.08}\n",
      "{'loss': 0.9633, 'grad_norm': 7.70405387878418, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.15}\n",
      "{'loss': 0.983, 'grad_norm': 10.537549018859863, 'learning_rate': 1.8484848484848487e-05, 'epoch': 0.23}\n",
      "{'loss': 0.9486, 'grad_norm': 3.4349822998046875, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.3}\n",
      "{'loss': 0.8945, 'grad_norm': 3.660184621810913, 'learning_rate': 1.7474747474747475e-05, 'epoch': 0.38}\n",
      "{'loss': 0.9968, 'grad_norm': 6.397351264953613, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.45}\n",
      "{'loss': 1.0389, 'grad_norm': 3.283048391342163, 'learning_rate': 1.6464646464646466e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9613, 'grad_norm': 5.859060287475586, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.61}\n",
      "{'loss': 0.989, 'grad_norm': 4.006072521209717, 'learning_rate': 1.5454545454545454e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9185, 'grad_norm': 6.259837627410889, 'learning_rate': 1.4949494949494952e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0707, 'grad_norm': 4.240743160247803, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n",
      "{'loss': 1.011, 'grad_norm': 4.7179155349731445, 'learning_rate': 1.3939393939393942e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9903, 'grad_norm': 3.596081018447876, 'learning_rate': 1.3434343434343436e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab0856b455f4875a91d017bbe25a485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9830149412155151, 'eval_runtime': 29.8044, 'eval_samples_per_second': 8.891, 'eval_steps_per_second': 1.141, 'epoch': 1.0}\n",
      "{'loss': 1.0235, 'grad_norm': 2.4713938236236572, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.06}\n",
      "{'loss': 0.9829, 'grad_norm': 2.539842367172241, 'learning_rate': 1.2424242424242425e-05, 'epoch': 1.14}\n",
      "{'loss': 0.9567, 'grad_norm': 3.476973533630371, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.21}\n",
      "{'loss': 1.068, 'grad_norm': 5.59661865234375, 'learning_rate': 1.1414141414141415e-05, 'epoch': 1.29}\n",
      "{'loss': 1.0023, 'grad_norm': 3.5393593311309814, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.36}\n",
      "{'loss': 0.9332, 'grad_norm': 5.222990036010742, 'learning_rate': 1.0404040404040405e-05, 'epoch': 1.44}\n",
      "{'loss': 0.8812, 'grad_norm': 3.184427499771118, 'learning_rate': 9.8989898989899e-06, 'epoch': 1.52}\n",
      "{'loss': 0.9933, 'grad_norm': 2.643828868865967, 'learning_rate': 9.393939393939396e-06, 'epoch': 1.59}\n",
      "{'loss': 1.0486, 'grad_norm': 6.812561988830566, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.67}\n",
      "{'loss': 0.9429, 'grad_norm': 4.937647342681885, 'learning_rate': 8.383838383838384e-06, 'epoch': 1.74}\n",
      "{'loss': 0.9979, 'grad_norm': 3.967308521270752, 'learning_rate': 7.87878787878788e-06, 'epoch': 1.82}\n",
      "{'loss': 0.944, 'grad_norm': 2.058410406112671, 'learning_rate': 7.373737373737374e-06, 'epoch': 1.89}\n",
      "{'loss': 0.9707, 'grad_norm': 4.910506248474121, 'learning_rate': 6.868686868686869e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7182cf8a2cb340cabf22cd9fa57ed9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9684506058692932, 'eval_runtime': 31.0305, 'eval_samples_per_second': 8.54, 'eval_steps_per_second': 1.096, 'epoch': 2.0}\n",
      "{'loss': 1.0659, 'grad_norm': 6.061112403869629, 'learning_rate': 6.363636363636364e-06, 'epoch': 2.05}\n",
      "{'loss': 0.9926, 'grad_norm': 3.2398083209991455, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.12}\n",
      "{'loss': 0.9473, 'grad_norm': 3.6849136352539062, 'learning_rate': 5.353535353535354e-06, 'epoch': 2.2}\n",
      "{'loss': 0.9115, 'grad_norm': 2.412933111190796, 'learning_rate': 4.848484848484849e-06, 'epoch': 2.27}\n",
      "{'loss': 1.0707, 'grad_norm': 7.528298854827881, 'learning_rate': 4.343434343434344e-06, 'epoch': 2.35}\n",
      "{'loss': 0.9221, 'grad_norm': 2.6718850135803223, 'learning_rate': 3.8383838383838385e-06, 'epoch': 2.42}\n",
      "{'loss': 0.9357, 'grad_norm': 3.609076499938965, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
      "{'loss': 1.0159, 'grad_norm': 6.547508239746094, 'learning_rate': 2.8282828282828286e-06, 'epoch': 2.58}\n",
      "{'loss': 0.9081, 'grad_norm': 2.826096534729004, 'learning_rate': 2.3232323232323234e-06, 'epoch': 2.65}\n",
      "{'loss': 0.9226, 'grad_norm': 2.5229358673095703, 'learning_rate': 1.8181818181818183e-06, 'epoch': 2.73}\n",
      "{'loss': 0.9869, 'grad_norm': 6.244053840637207, 'learning_rate': 1.3131313131313134e-06, 'epoch': 2.8}\n",
      "{'loss': 0.9789, 'grad_norm': 2.713515281677246, 'learning_rate': 8.080808080808082e-07, 'epoch': 2.88}\n",
      "{'loss': 0.9933, 'grad_norm': 7.009163856506348, 'learning_rate': 3.0303030303030305e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bf46d51831495bb2014b58f1c16b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9636731743812561, 'eval_runtime': 30.6682, 'eval_samples_per_second': 8.641, 'eval_steps_per_second': 1.109, 'epoch': 3.0}\n",
      "{'train_runtime': 1332.0663, 'train_samples_per_second': 2.378, 'train_steps_per_second': 0.297, 'train_loss': 0.9789928498894277, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c791c8eb00540a1b7bfa0bd9f2dce04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa model accuracy: 0.51\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        31\n",
      "     neutral       0.51      1.00      0.67       134\n",
      "    positive       0.00      0.00      0.00       100\n",
      "\n",
      "    accuracy                           0.51       265\n",
      "   macro avg       0.17      0.33      0.22       265\n",
      "weighted avg       0.26      0.51      0.34       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and train the model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=3\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Decode predictions and calculate accuracy\n",
    "decoded_preds = encoder.inverse_transform(preds)\n",
    "decoded_labels = encoder.inverse_transform(y_test)\n",
    "\n",
    "accuracy = accuracy_score(decoded_labels, decoded_preds)\n",
    "print(f\"RoBERTa model accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        decoded_labels, decoded_preds, target_names=encoder.classes_, zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models/roberta_label_encoder.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the save path\n",
    "save_path = \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models\"\n",
    "\n",
    "# Save the trained model\n",
    "roberta_model.save_pretrained(save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Save the LabelEncoder as a pickle object\n",
    "joblib.dump(\n",
    "    encoder,\n",
    "    f\"{save_path}/roberta_label_encoder.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "nb_classifier = joblib.load(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models/custom_nb_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.43018867924528303\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        31\n",
      "     neutral       0.47      0.67      0.55       134\n",
      "    positive       0.33      0.24      0.28       100\n",
      "\n",
      "    accuracy                           0.43       265\n",
      "   macro avg       0.27      0.30      0.28       265\n",
      "weighted avg       0.36      0.43      0.39       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to SAME, UNALTERED custom Naive Bayes model from above applied to this test data subset.\n",
    "# Extra testing!\n",
    "# predict\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# decode\n",
    "custom_nb_pred = encoder.inverse_transform(y_pred)\n",
    "\n",
    "y_test_decoded = encoder.inverse_transform(y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test_decoded, custom_nb_pred)\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_decoded, custom_nb_pred, \n",
    "        target_names=encoder.classes_, \n",
    "        zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare, contrast and discuss these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
