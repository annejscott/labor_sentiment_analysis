{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained roBERTa embeddings as the initial embedding to pretrain the model using contrastive loss to learn the embeddings, create, train and run a Deep Clustering by Semantic Contrastive Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full, now labelled data from the file\n",
    "reddit = joblib.load(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/pickle/reddit_labelled.pkl\"\n",
    ")\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "save_path = \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models/roBERTa\"\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(save_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "roberta_model.eval()\n",
    "\n",
    "# Tokenize and encode\n",
    "def tokenize_texts(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_inputs = tokenize_texts(reddit['text_processed'].tolist(), tokenizer)\n",
    "\n",
    "## DATA PREPROCESSING COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips for Effective Implementation\n",
    "\n",
    "    Batch Sampling: Ensure diverse positive and negative pairs in each batch for effective contrastive learning.\n",
    "    Augmentations: Use augmentations (e.g., synonym replacement, backtranslation) to create robust positive pairs.\n",
    "    Initialization: Use K-Means for an initial clustering of embeddings.\n",
    "    Hyperparameters:\n",
    "        Tune ττ (temperature) in contrastive loss.\n",
    "        Experiment with the projection dimension in the projection head.\n",
    "        Adjust λλ to balance losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "# inputs fed in batches\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs):\n",
    "        self.input_ids = tokenized_inputs[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "        }\n",
    "\n",
    "# Initialize dataset and loader\n",
    "dataset = RedditDataset(tokenized_inputs)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify using ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Clustering by Semantic Constrastive Learning Model\n",
    "class DCSCModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_clusters=10, projection_dim=128):\n",
    "        super(DCSCModel, self).__init__()\n",
    "        self.encoder = pretrained_model.roberta # RoBERTa encoder\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_dim, projection_dim),\n",
    "        )\n",
    "        self.cluster_head = nn.Linear(projection_dim, num_clusters)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # extract embeddings\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # pass embeddings through projection and clustering heads\n",
    "        projections = self.projection_head(cls_embeddings)\n",
    "        clusters = self.cluster_head(projections)\n",
    "        return projections, clusters\n",
    "\n",
    "# Contrastive Loss\n",
    "# cosine similarity and temp parameter\n",
    "def contrastive_loss(projections, temperature=0.1):\n",
    "    # normalize projections\n",
    "    projections = F.normalize(projections, dim=1)\n",
    "    similarity_matrix = torch.mm(projections, projections.T) / temperature\n",
    "    labels = torch.arange(len(projections)).to(projections.device)\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return loss\n",
    "\n",
    "# Clustering Loss\n",
    "# kl divergence loss\n",
    "def clustering_loss(cluster_logits, target_distribution, batch_indices):\n",
    "    cluster_probs = F.softmax(cluster_logits, dim=1)\n",
    "    target_dist_batch = target_distribution[batch_indices]\n",
    "    return F.kl_div(cluster_probs.log(), target_dist_batch, reduction=\"batchmean\")\n",
    "\n",
    "# Combined Loss\n",
    "# balances with lambda weight\n",
    "def compute_loss(contrastive_loss, clustering_loss, lambda_weight=0.5):\n",
    "    return lambda_weight * contrastive_loss + (1 - lambda_weight) * clustering_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataloader, optimizer, num_clusters, epochs=10, lambda_weight=0.5):\n",
    "    model.train()\n",
    "    \n",
    "    # initialize k-means for initial embeddings\n",
    "    embeddings_list = []\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            projections, _ = model(input_ids, attention_mask)\n",
    "            embeddings_list.append(projections)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings_list)\n",
    "    kmeans = KMeans(n_clusters=num_clusters).fit(embeddings.cpu().numpy())\n",
    "    target_distribution = torch.tensor(kmeans.transform(embeddings.cpu().numpy())).to(embeddings.device)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            # forward\n",
    "            projections, clusters = model(input_ids, attention_mask)\n",
    "\n",
    "            # compute loss\n",
    "            loss_contrastive = contrastive_loss(projections)\n",
    "\n",
    "            batch_indices = torch.arange(batch_idx * len(batch['input_ids']), \n",
    "                                         (batch_idx + 1) * len(batch['input_ids'])).to(projections.device)\n",
    "            lost_cluster = clustering_loss(clusters, target_distribution, batch_indices)\n",
    "            loss = compute_loss(loss_contrastive, lost_cluster, lambda_weight)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # recompute target distro using kmeans\n",
    "        if epoch % 2 == 0:\n",
    "            embeddings_list = []\n",
    "            for batch in dataloader:\n",
    "                with torch.no_grad():\n",
    "                    input_ids = batch[\"input_ids\"]\n",
    "                    attention_mask = batch[\"attention_mask\"]\n",
    "                    projections, _ = model(input_ids, attention_mask)\n",
    "                    embeddings_list.append(projections)\n",
    "\n",
    "            embeddings = torch.cat(embeddings_list)\n",
    "            kmeans = KMeans(n_clusters=num_clusters).fit(embeddings.cpu().numpy())\n",
    "            target_distribution = torch.tensor(kmeans.transform(embeddings.cpu().numpy())).to(embeddings.device)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "num_clusters = 10\n",
    "projection_dim = 128\n",
    "model = DCSCModel(\n",
    "    roberta_model, \n",
    "    num_clusters=num_clusters, \n",
    "    projection_dim=projection_dim\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.159804344177246\n",
      "Epoch 2, Loss: 13.22829532623291\n",
      "Epoch 3, Loss: 13.227643966674805\n",
      "Epoch 4, Loss: 9.851664543151855\n",
      "Epoch 5, Loss: 9.850968360900879\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, dataloader, optimizer, num_clusters, epochs=10, lambda_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Define the save path\n",
    "save_folder = \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models/DCSC\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Save the model state dictionary\n",
    "model_save_path = os.path.join(save_folder, \"dcsc_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = os.path.join(save_folder, \"tokenizer.pkl\")\n",
    "joblib.dump(tokenizer, tokenizer_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and Comparison\n",
    "\n",
    "    Compare results with K-Means on pre-trained embeddings and DeepCluster.\n",
    "    Analyze cluster quality using NMI, ARI, and visualize with t-SNE/UMAP.\n",
    "    Use Latent Dirichlet Allocation (LDA) or Dynamic Topic Modeling (DTM) to explore cluster themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defend softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Assignments\n",
    "def get_cluster_assignments(model, dataloader):\n",
    "    model.eval()\n",
    "    all_clusters = []\n",
    "    all_projections = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            projections, cluster_logits = model(input_ids, attention_mask)\n",
    "            cluster_probs = F.softmax(cluster_logits, dim=1)\n",
    "            cluster_assignments = torch.argmax(cluster_probs, dim=1)\n",
    "\n",
    "            all_clusters.append(cluster_assignments.cpu())\n",
    "            all_projections.append(projections.cpu())\n",
    "            all_inputs.extend(input_ids.cpu())\n",
    "    return torch.cat(all_clusters), torch.cat(all_projections), all_inputs\n",
    "\n",
    "cluster_assignments, projections, inputs = get_cluster_assignments(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clusters\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# reduce dimentions\n",
    "pca = PCA(n_components=2)\n",
    "reduced_projections = pca.fit_transform(projections.numpy())\n",
    "\n",
    "# scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    reduced_projections[:, 0], \n",
    "    reduced_projections[:, 1],\n",
    "    c=cluster_assignments, \n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(\"Cluster Visualization\")\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine each cluster\n",
    "texts = reddict['text_processed'].tolist()\n",
    "clusters = cluster_assignments.numpy()\n",
    "\n",
    "# group by cluster\n",
    "cluster_texts = {i: [] for i in range(num_clusters)}\n",
    "for text, cluster in zip(texts, clusters):\n",
    "    cluster_texts[cluster].append(text)\n",
    "\n",
    "for cluster_id, texts in cluster_texts.items():\n",
    "    print(f\"Cluster {cluster_id}\")\n",
    "    print(\"Example texts:\")\n",
    "    print(\"\\n\".join(texts[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Extraction\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "cluster_keywords = {}\n",
    "for cluster_id, texts in cluster_texts.items():\n",
    "    all_words = [word for text in texts for word in word_tokenize(text)]\n",
    "    cluster_keywords[cluster_id] = Counter(all_words).most_common(10)\n",
    "\n",
    "for cluster_id, keywords in cluster_keywords.items():\n",
    "    print(f\"Cluster {cluster_id} Keywords:\")\n",
    "    print(\", \".join([word for word, count in keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare labels in clusters\n",
    "import pandas as pd\n",
    "\n",
    "labels_nb = reddit[\"label_nb\"].tolist()\n",
    "labels_rob = reddit[\"label_rob\"].tolist()\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\"text\": texts, \n",
    "     \"label_nb\": labels_nb, \n",
    "     \"label_rob\": labels_rob, \n",
    "     \"cluster\": clusters}\n",
    ")\n",
    "# Calculate mean sentiment for each cluster by model\n",
    "cluster_sentiments = data.groupby(\"cluster\")[[\"label_nb\", \"label_rob\"]].mean()\n",
    "\n",
    "# Count sentiment distribution (e.g., positive, neutral, negative) for each model\n",
    "cluster_distribution = (\n",
    "    data.groupby(\"cluster\")[[\"label_nb\", \"label_rob\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Average Sentiments by Cluster:\")\n",
    "print(cluster_sentiments)\n",
    "\n",
    "print(\"\\nSentiment Distribution by Cluster:\")\n",
    "print(cluster_distribution)\n",
    "\n",
    "\n",
    "# Plot mean sentiment scores for each model\n",
    "cluster_sentiments.plot(kind=\"bar\", figsize=(12, 6), alpha=0.8)\n",
    "plt.title(\"Mean Sentiment Scores by Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Mean Sentiment\")\n",
    "plt.legend([\"Label NB\", \"Label Rob\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Plot sentiment distributions\n",
    "for model in [\"label_nb\", \"label_rob\"]:\n",
    "    distribution = cluster_distribution[model]\n",
    "    distribution.plot(kind=\"bar\", stacked=True, figsize=(12, 6), alpha=0.8)\n",
    "    plt.title(f\"Sentiment Distribution for {model} by Cluster\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.legend([\"Negative\", \"Neutral\", \"Positive\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# Add 'agree' column\n",
    "data[\"agreement\"] = data[\"label_nb\"] == data[\"label_rob\"]\n",
    "\n",
    "# Calculate agreement percentage in each cluster\n",
    "agreement_by_cluster = data.groupby(\"cluster\")[\"agreement\"].mean() * 100\n",
    "\n",
    "print(\"Agreement Between Models by Cluster (%):\")\n",
    "print(agreement_by_cluster)\n",
    "\n",
    "# Visualize agreement\n",
    "agreement_by_cluster.plot(kind=\"bar\", figsize=(12, 6), color=\"skyblue\", alpha=0.8)\n",
    "plt.title(\"Agreement Between Models by Cluster (%)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Agreement (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_texts(texts):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    preprocessed_texts = []\n",
    "    for text in texts:\n",
    "        # Tokenize and remove punctuation\n",
    "        tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "        # Remove stop words\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        preprocessed_texts.append(filtered_tokens)\n",
    "    return preprocessed_texts\n",
    "\n",
    "\n",
    "# Preprocess texts\n",
    "texts = reddit[\"text\"].tolist()\n",
    "preprocessed_texts = preprocess_texts(texts)\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Fit LDA model\n",
    "num_topics = 10  # Number of topics\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42\n",
    ")\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for idx, topic in topics:\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Prepare the visualization\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.show(lda_vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
