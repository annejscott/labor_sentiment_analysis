{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually labelled 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/train/reddit_labeled.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "neutral     222\n",
      "positive    158\n",
      "negative     47\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reddit['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_h          0         1         2\n",
      "label                                 \n",
      "negative  0.978723  0.021277  0.000000\n",
      "neutral   0.837838  0.157658  0.004505\n",
      "positive  0.955696  0.031646  0.012658\n",
      "label_k          0         1         2\n",
      "label                                 \n",
      "negative  0.595745  0.042553  0.361702\n",
      "neutral   0.310811  0.256757  0.432432\n",
      "positive  0.588608  0.044304  0.367089\n"
     ]
    }
   ],
   "source": [
    "# relationships between the label columns\n",
    "print(pd.crosstab(reddit[\"label\"], reddit[\"label_h\"], normalize='index'))\n",
    "print(pd.crosstab(reddit[\"label\"], reddit[\"label_k\"], normalize='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit    category\n",
      "post_id        object\n",
      "text           object\n",
      "score           int64\n",
      "year         category\n",
      "month        category\n",
      "day          category\n",
      "label_h      category\n",
      "label_k      category\n",
      "label        category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# labels as factors\n",
    "reddit = f.reddit_dtypes(reddit)\n",
    "\n",
    "# check work\n",
    "print(reddit.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsample to balance classes. apply cross validation for stronger performance. Random forest can use small datasets, but if the sample is too small to adequetely capture larger ptterns. RF can underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test and train\n",
    "train = reddit.dropna(subset=['label'])\n",
    "test = reddit[reddit['label'].isna()]\n",
    "\n",
    "X_train = train[['label_h', 'label_k']] \n",
    "y_train = train[\"label\"]\n",
    "\n",
    "X_test = test[[\"label_h\", \"label_k\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample  to reduse majority classes\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV accuracy: 0.3830049261083744\n"
     ]
    }
   ],
   "source": [
    "# build model and predict\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "cv_scores = cross_val_score(clf, X_resampled, y_resampled, \n",
    "                            cv=5,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "print(f\"Mean CV accuracy: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "predicted_labels = encoder.inverse_transform(predicted)\n",
    "\n",
    "reddit.loc[reddit[\"label\"].isna(), \"label_rf\"] = predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit['label_rf'] = reddit['label_rf'].fillna(reddit['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv(\n",
    "    \"/Users/seshat/Documents/GitHub/labor_sentiment_analysis/data/train/reddit_predicted.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
